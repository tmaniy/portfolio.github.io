{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eJdqzltvaCtiEpvtBl_3dX2ipQkVFY7O",
      "authorship_tag": "ABX9TyPo7mTAGSk0Eynybig/wLtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmaniy/portfolio.github.io/blob/main/Data_Extraction_and_NLP(TEST_ASSIGNMENT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WEB SCRAPING"
      ],
      "metadata": {
        "id": "qOBHHXNP4IDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install goose3"
      ],
      "metadata": {
        "id": "HYo1aYeJW80L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from goose3 import Goose\n",
        "import nltk"
      ],
      "metadata": {
        "id": "FZ-_vS0aE4BM"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_url(url):\n",
        "    # Create a Goose instance\n",
        "    g = Goose()\n",
        "\n",
        "    # Fetch the article content\n",
        "    article = g.extract(url=url)\n",
        "\n",
        "    # Get the cleaned text\n",
        "    cleaned_text = article.cleaned_text\n",
        "\n",
        "    return cleaned_text\n",
        "url = 'https://insights.blackcoffer.com/how-machine-learning-will-affect-your-business/'\n",
        "article_text = extract_text_from_url(url)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1oh_9GwfFls5"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DELETING .ipynb_checkpoints directory which was creating problem"
      ],
      "metadata": {
        "id": "KGE_PmcN4M3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Specify the path to the folder containing the .ipynb_checkpoints directory\n",
        "folder_path = 'stop_words_file'\n",
        "\n",
        "# Combine the folder path with the .ipynb_checkpoints directory\n",
        "checkpoints_path = os.path.join(folder_path, '.ipynb_checkpoints')\n",
        "\n",
        "# Check if the .ipynb_checkpoints directory exists\n",
        "if os.path.exists(checkpoints_path):\n",
        "    # Delete the .ipynb_checkpoints directory\n",
        "    shutil.rmtree(checkpoints_path)\n",
        "    print(f\"{checkpoints_path} deleted successfully.\")\n",
        "else:\n",
        "    print(f\"{checkpoints_path} does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF6M1Y00SKSS",
        "outputId": "ebd60440-4abf-4be6-c1ef-a2a3c34ea9ce"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stop_words_file/.ipynb_checkpoints does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UPLOAD STOPWORDS FOLDER AND REMOVING THEM FROM CLEAN TEXT EXTRACTED FROM URL"
      ],
      "metadata": {
        "id": "Q83-x1uO4WIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "def read_stop_words_from_files(folder_path , encoding='ISO-8859-1'):\n",
        "    stop_words = set()\n",
        "\n",
        "    # Iterate through files in the specified folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read stop words from each file\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=encoding) as file:\n",
        "                stop_words.update(file.read().splitlines())\n",
        "        except UnicodeDecodeError as e:\n",
        "            print(f\"Error reading file '{file_path}': {e}\")\n",
        "\n",
        "    return stop_words\n",
        "\n",
        "def remove_stop_words(article_text, stop_words):\n",
        "    # Tokenize the article text into words\n",
        "    words = word_tokenize(article_text)\n",
        "\n",
        "    # Remove stop words from the list of words\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Reconstruct the text without stop words\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "# Folder path containing stop words files\n",
        "stop_words_folder_path = 'stop_words_file'\n",
        "\n",
        "# Read stop words from multiple files in the folder\n",
        "stop_words = read_stop_words_from_files(stop_words_folder_path)\n",
        "\n",
        "\n",
        "\n",
        "# Example article text\n",
        "article_text = article_text\n",
        "\n",
        "# Remove stop words from the article text\n",
        "filtered_text = remove_stop_words(article_text, stop_words)\n",
        "\n",
        "# Print the result\n",
        "print(\"Original Article Text:\")\n",
        "print(article_text)\n",
        "print(\"\\nFiltered Text (without stop words):\")\n",
        "print(filtered_text)\n"
      ],
      "metadata": {
        "id": "VyRtnFyXENP5"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CALCULATING SENTIMENTAL SCORES"
      ],
      "metadata": {
        "id": "G-1NrG1ZJ4MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def read_word_file(file_path , encoding='ISO-8859-1'):\n",
        "    with open(file_path, 'r', encoding=encoding) as file:\n",
        "        words = file.read().splitlines()\n",
        "    return set(words)\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    return cleaned_text\n",
        "\n",
        "def calculate_scores(text, positive_dict, negative_dict):\n",
        "    cleaned_text = clean_text(text)\n",
        "    words = cleaned_text.split()\n",
        "\n",
        "    positive_score = sum(1 for word in words if word in positive_dict)\n",
        "    negative_score = sum(1 for word in words if word in negative_dict)\n",
        "\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
        "\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
        "\n",
        "# File paths for positive and negative dictionaries\n",
        "positive_file_path = 'positive-words.txt'\n",
        "negative_file_path = 'negative-words.txt'\n",
        "\n",
        "\n",
        "positive_dict = read_word_file(positive_file_path)\n",
        "negative_dict = read_word_file(negative_file_path)\n",
        "\n",
        "\n",
        "input_text_variable = filtered_text\n",
        "\n",
        "# Calculate scores using the input text variable\n",
        "pos_score, neg_score, pol_score, subj_score = calculate_scores(input_text_variable, positive_dict, negative_dict)\n",
        "\n",
        "\n",
        "print(f\"Positive Score: {pos_score}\")\n",
        "print(f\"Negative Score: {neg_score}\")\n",
        "print(f\"Polarity Score: {pol_score}\")\n",
        "print(f\"Subjectivity Score: {subj_score}\")\n"
      ],
      "metadata": {
        "id": "-KVnClj_J5-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CALCULATING OTHER REMAINING VARIABLES"
      ],
      "metadata": {
        "id": "urtaRkkV4pr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import words\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "\n",
        "#defining function for each variable whose value has to be computed\n",
        "def calculate_complex_words(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    complex_words = [word for word in word_tokens if len(word) > 6 and word.lower() in words.words()]\n",
        "    return len(complex_words)\n",
        "\n",
        "def calculate_average_sentence_length(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
        "    total_sentences = len(sentences)\n",
        "    return total_words / total_sentences\n",
        "\n",
        "def calculate_complex_words_percentage(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    total_words = len(word_tokens)\n",
        "    complex_words = calculate_complex_words(text)\n",
        "    return (complex_words / total_words) * 100\n",
        "\n",
        "def calculate_fog_index(avg_sentence_length, complex_words_percentage):\n",
        "    return 0.4 * (avg_sentence_length + complex_words_percentage)\n",
        "\n",
        "def calculate_average_sentence_length(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
        "    total_sentences = len(sentences)\n",
        "    return total_words / total_sentences\n",
        "\n",
        "def calculate_word_count(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_text = clean_text(text)\n",
        "    words = [word for word in cleaned_text.split() if word not in stop_words]\n",
        "    return len(words)\n",
        "\n",
        "def calculate_syllable_count_per_word(word):\n",
        "    vowels = \"aeiouy\"\n",
        "    count = 0\n",
        "    word = word.lower()\n",
        "\n",
        "    # Exceptions for syllable count\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        pass\n",
        "    elif word.endswith(\"e\"):\n",
        "        if word != \"the\":\n",
        "            count += 1\n",
        "    else:\n",
        "        for char in word:\n",
        "            if char in vowels:\n",
        "                count += 1\n",
        "\n",
        "    return count\n",
        "\n",
        "def calculate_syllable_count(text):\n",
        "    words = word_tokenize(text)\n",
        "    syllable_count = sum(calculate_syllable_count_per_word(word) for word in words)\n",
        "    return syllable_count\n",
        "\n",
        "def calculate_personal_pronouns(text):\n",
        "    pronoun_list = ['I', 'we', 'my', 'ours', 'us']\n",
        "    # Use regex to find counts of personal pronouns\n",
        "    pronoun_count = sum(1 for word in pronoun_list if re.search(r'\\b' + re.escape(word) + r'\\b', text, flags=re.IGNORECASE))\n",
        "    return pronoun_count\n",
        "\n",
        "def calculate_average_word_length(text):\n",
        "    words = word_tokenize(text)\n",
        "    total_characters = sum(len(word) for word in words)\n",
        "    return total_characters / len(words)\n",
        "\n",
        "\n",
        "input_text_variable = filtered_text\n",
        "\n",
        "\n",
        "avg_sentence_length = calculate_average_sentence_length(input_text_variable)\n",
        "complex_words_percentage = calculate_complex_words_percentage(input_text_variable)\n",
        "fog_index = calculate_fog_index(avg_sentence_length, complex_words_percentage)\n",
        "avg_sentence_length = calculate_average_sentence_length(input_text_variable)\n",
        "complex_words_count = calculate_complex_words(input_text_variable)\n",
        "word_count = calculate_word_count(input_text_variable)\n",
        "syllable_count = calculate_syllable_count(input_text_variable)\n",
        "personal_pronouns_count = calculate_personal_pronouns(input_text_variable)\n",
        "avg_word_length = calculate_average_word_length(input_text_variable)\n",
        "\n",
        "avg_words_per_sentence = word_count / len(sent_tokenize(input_text_variable))\n",
        "\n",
        "print(f\"Average Sentence Length: {avg_sentence_length}\")\n",
        "print(f\"Percentage of Complex Words: {complex_words_percentage}\")\n",
        "print(f\"Fog Index: {fog_index}\")\n",
        "print(f\"Average Sentence Length: {avg_sentence_length}\")\n",
        "print(f\"Complex Words Count: {complex_words_count}\")\n",
        "print(f\"Word Count: {word_count}\")\n",
        "print(f\"Syllable Count: {syllable_count}\")\n",
        "print(f\"Personal Pronouns Count: {personal_pronouns_count}\")\n",
        "print(f\"Average Word Length: {avg_word_length}\")\n",
        "print(f\"Average Number of Words Per Sentence: {avg_words_per_sentence}\")\n"
      ],
      "metadata": {
        "id": "PNZcXjuiWpIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE CAN CALCULATE  VALUE OF VARIABLES FOR EACH URL ONE BY ONE .\n",
        "I HAVE CALCULATED THE VARIABLES TILL 40TH ARTICLE AND FOR REMAINING ARTICLES WE CAN PASTE THE URL IN THE URL SECTION AND FIND THE RESULTS"
      ],
      "metadata": {
        "id": "fG88ZQDR4tXL"
      }
    }
  ]
}